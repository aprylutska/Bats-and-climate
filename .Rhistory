check_model(mod_Ra_all) %>% plot()
check_model(mod_Ra_all, panel = FALSE) %>% plot()
png("./figures/check_model.png", width = 26, height = 22, units = "cm", res = 300)
check_model(mod_Ra_all, panel = FALSE) %>% plot()
dev.off()
check_model(mod_Ra_all, panel = FALSE) |> plot()
check_model(mod_Ra_all, panel = FALSE) %>%  plot()
rm(list = ls()) # reset R's brain
library(lubridate)
library(ggplot2)
library(tidyverse) # for data manipulations
library(easyclimate) # for daily climate data extraction
library(sf) # for spatial operations
library(gridExtra) # for ggplot plates
library(jtools)
# Set custom theme for ggplot2 plots
mytheme <- theme_bw() +
theme(panel.grid.major = element_blank(),
panel.grid.minor = element_blank())
# Set custom theme as a default for all ggplot below
theme_set(mytheme)
# if LC_TIME=uk_UA.UTF-8, your months will be in Ukrainian.
# To fix that, you need to re-map locate into US (will be back after restart R session)
# Set locale to get English months names
# Sys.setlocale("LC_TIME", "en_US.UTF-8") # for Linux
# Sys.setlocale("LC_TIME", "English") # for Windows
# Custom function to check dependency of selected growth metric on day across years
source("./functions/check_my_var.R")
# Custom function that calculate Growing degree units (GDU, Sum of the mean daily temperature above 0 from the start of the year) for each day of the year.
source("./functions/gdu.R")
# Custom function to find spring onset.
source("./functions/spring_onset_day.R")
# Read data and prepare it for further analyses
df <- read.csv("Homolsha_all_years.csv") # read data from *.csv file
# Set variables classes
df$date <- as.Date(df$Data, "%d.%m.%Y") # convert date to R Data format
df$Place <- as.factor(df$Place)
df$Species <- as.factor(df$Species)
df$sex <- as.factor(df$sex)
df$age <- as.factor(df$age)
df$Year <- year(df$date) #виокремлення року
df$Year <-as.factor(df$Year)
#створення нової колонки з даними про день від початку року
df$day <- yday(df$date)
df$Ra <- sub(",", ".", df$Ra) #заміна ком на крапки в промірах передпліччя
df$W <- sub(",", ".", df$W)
df$Ra <-as.numeric (df$Ra) #присвоєння промірам нумеричного типу даних
df$W <-as.numeric (df$W)
# Delete unused variables
df$Data <- NULL
df$reproductive.status <- NULL
# Drop rows with NAs in W and Ra
df <- df[!is.na(df$W) & !is.na(df$Ra),]
# Loaded packages with their versions
sessionInfo() # to check your locale
summary(df)
table (df$Species,df$age)
# Subset only some species (e.g., N.noc. , M.daubentonii)
df <- subset(df,
Species %in% c('Nyctalus noctula'
#,'Myotis daubentonii'
) &
age == "sad"
)
summary (df)
df <- df %>%
# count bats per day
count(date) %>%
# choose days where more than 10 bats
filter(n > 10) %>%
# subset with days where more than 10 bats
inner_join(df, by = join_by(date)) %>%
select(-n)
ggplot(data = df, aes(Ra, W
, colour = Year
)
) +
geom_point(alpha = 0.5) +
geom_smooth(method = "lm", aes(fill = Year)) +
labs(x = "Forearm length, mm",
y = "Weight, g")
p1 <- ggplot(data = df, aes(sex, Ra)) +
geom_violin() +
labs(title = "Forearm length",
y = "Forearm length, mm")
p2 <- ggplot(data = df, aes(sex, W)) +
geom_violin() +
labs(title = "Weight",
y = "Weight, g")
gridExtra::grid.arrange(p1, p2, nrow=1, ncol=2)
p1 <- ggplot(data = df, aes(Year, Ra)) +
geom_violin() +
labs(title = "Forearm length",
y = "Ra, mm")
p2 <- ggplot(data = df, aes(Year, W)) +
geom_violin() +
labs(title = "Weight",
y = "W, g")
gridExtra::grid.arrange(p1, p2, nrow=1, ncol=2)
# t ~ Ra
p_Ra_day_4years <- ggplot(data = df, aes(x = day, y = Ra, colour = Year)) +
geom_point() +
geom_smooth(method = "lm", aes(fill = Year)) +
labs(title = "Ra ~ DOY",
x = "Day of the year",
y = "Forearm length, mm")
# t ~ W
p_W_day_4years <- ggplot(data = df, aes(x = day, y = W, colour = Year)) +
geom_point() +
geom_smooth(method = "lm", aes(fill = Year)) +
labs(title = "W ~ DOY",
x = "Day of the year",
y = "Weight, g")
gridExtra::grid.arrange(p_Ra_day_4years, p_W_day_4years, nrow=1, ncol=2)
rm(p_Ra_day_4years, p_W_day_4years)
check_my_var(df, "Ra", "day")
check_my_var(df, "W", "day")
ggplot(df, aes(day)) +
geom_histogram() +
labs(x = "Day of the year",
y = "Number of bats") +
facet_wrap(vars(Year))
# Fit simple linear model
mod <- lm(log(W) ~ log(day), data = df)
# Get residuals
df$BCI <- residuals(mod)
# Drop unused variables
rm(mod)
check_my_var(df, "BCI", "day")
ggplot(data = df, aes(Year, BCI)) +
geom_violin() +
labs(y = "Body Condition Index")
ggplot(data = df, aes(day, BCI, colour = Year)) +
geom_point(alpha = 0.5) +
geom_smooth(method = "lm", aes(fill = Year)) +
labs(x = "Day of the year",
y = "Body Condition Index")
# facet_wrap(vars(sex))
# Drop 2014 due to the lack of successful nights
df <- df[df$Year != "2014", ]
# # Check for normality
# # if p value > 0.05 - accept Null Hypothesis, otherwise reject it
#
# # Assess normality for each year separately
# shapiro.test(df$Ra[df$Year == "2008"])
# shapiro.test(df$Ra[df$Year == "2011"])
# shapiro.test(df$Ra[df$Year == "2019"])
#
#
# shapiro.test(df$BCI[df$Year == "2008"])
# shapiro.test(df$BCI[df$Year == "2011"])
# shapiro.test(df$BCI[df$Year == "2019"])
# Bartlett test of homogeneity of variances
# If p > 0.05, accept Null Hypothesis (variance are homogeneous)
# Ra
barlett_Ra <- bartlett.test(Ra ~ Year, df)
# BCI
barlett_BCI <- bartlett.test(BCI ~ Year, df)
p1 <- ggplot(data = df, aes(x = Ra)) +
geom_histogram() +
labs(title = paste0("Bartlett test of homogeneity of variances: p-value = ",
round(barlett_Ra$p.value, digits = 3)),
x = "Foreadrm length, mm",
y = "Frequency") +
facet_wrap(vars(Year))
p2 <- ggplot(data = df, aes(x = BCI)) +
geom_histogram() +
labs(title = paste0("Bartlett test of homogeneity of variances: p-value = ",
round(barlett_BCI$p.value, digits = 3)),
x = "Body Condition Index",
y = "Frequency") +
facet_wrap(vars(Year))
gridExtra::grid.arrange(p1, p2, nrow=2, ncol=1)
# Compute the analysis of variance
res.aov.Ra <- aov(Ra ~ Year, data = df)
summary(res.aov.Ra)
# H0: All group means are equal
# HA: Not all group means are equal
# The F value column is the test statistic from the F test. This is the mean square of each independent variable divided by the mean square of the residuals. The larger the F value, the more likely it is that the variation caused by the independent variable is real and not due to chance.
# The Pr(>F) column is the p value of the F statistic. This shows how likely it is that the F value calculated from the test would have occurred if the null hypothesis of no difference among group means were true.
ggplot(df, aes(x = Year, y = Ra, colour = Year)) +
geom_boxplot() +
scale_color_manual(values = c("#00AFBB", "#E7B800", "#FC4E07")) +
annotate("text",        # type
x = 1, y = 49, # position
hjust = 0,     # alignment left
label = paste(paste0("F = ", round(summary(res.aov.Ra)[[1]][["F value"]][1], digits = 3)),
paste0("p = ", round(summary(res.aov.Ra)[[1]][["Pr(>F)"]][1], digits = 3)),
sep = "\n"),
family = "serif", fontface = "italic", size = 5) +
labs(title = "Differences in forearm length for Noctule bats,\nOne-way Analysis of Variance",
x = "", y = "Forearm length, mm",
colour = "") +
# scale_colour_discrete(labels = c("I. setosa", "I. versicolor", "I. virginica")) +
theme(axis.text.x = element_blank(),
axis.ticks.x = element_blank(),
# match legend key to panel.background
legend.key       = element_blank(),
legend.text = element_text(face = "italic", size = 12),
legend.position = "bottom")
# Tukey Honest Significant Differences
TukeyHSD(res.aov.Ra, conf.level = .95)
# if adjusted p-value lesser than 0.05 - pair differs significantly
# Plotting ANOVA assumptions
par(mfrow = c(1, 2))
# Check ANOVA assumptions
# 1. Homogeneity of variances (The residuals versus fits plot)
plot(res.aov.Ra, 1)
# 2. Normality plot of residuals
plot(res.aov.Ra, 2)
par(mfrow = c(1, 1))
# Alternatively - Shapiro-Wilk test for the residuals
# Extract the residuals
aov_residuals <- residuals(object = res.aov.Ra)
# Run Shapiro-Wilk test
shapiro.test(x = aov_residuals)
# Compute the analysis of variance
res.aov.BCI <- aov(BCI ~ Year, data = df)
summary(res.aov.BCI)
# H0: All group means are equal
# HA: Not all group means are equal
# The F value column is the test statistic from the F test. This is the mean square of each independent variable divided by the mean square of the residuals. The larger the F value, the more likely it is that the variation caused by the independent variable is real and not due to chance.
# The Pr(>F) column is the p value of the F statistic. This shows how likely it is that the F value calculated from the test would have occurred if the null hypothesis of no difference among group means were true.
ggplot(df, aes(x = Year, y = BCI, colour = Year)) +
geom_boxplot() +
scale_color_manual(values = c("#00AFBB", "#E7B800", "#FC4E07")) +
# annotate("text",  x = 1, y = -0.4,
#          label = paste0("p = ", round(summary(res.aov.BCI)[[1]][["Pr(>F)"]][1], digits = 4)),
#          family = "serif", fontface = "italic", size = 5) +
annotate("text",        # type
x = 1, y = -0.4, # position
hjust = 0,     # alignment left
label = paste(paste0("F = ", round(summary(res.aov.BCI)[[1]][["F value"]][1], digits = 3)),
paste0("p = ", round(summary(res.aov.BCI)[[1]][["Pr(>F)"]][1], digits = 3)),
sep = "\n"),
family = "serif", fontface = "italic", size = 5) +
labs(title = "Differences in forearm length for Noctule bats,\nOne-way Analysis of Variance",
x = "", y = "BCI",
colour = "") +
# scale_colour_discrete(labels = c("I. setosa", "I. versicolor", "I. virginica")) +
theme(axis.text.x = element_blank(),
axis.ticks.x = element_blank(),
# match legend key to panel.background
legend.key       = element_blank(),
legend.text = element_text(face = "italic", size = 12),
legend.position = "bottom")
# Tukey Honest Significant Differences
TukeyHSD(res.aov.BCI, conf.level = .95)
# if adjusted p-value lesser than 0.05 - pair differs significantly
# Plotting ANOVA assumptions
par(mfrow=c(1,2))
# Check ANOVA assumptions
# 1. Homogeneity of variances (The residuals versus fits plot)
plot(res.aov.BCI, 1)
# 2. Normality plot of residuals
plot(res.aov.BCI, 2)
par(mfrow=c(1,1))
# Alternatively - Shapiro-Wilk test for the residuals
# ExtBCIct the residuals
aov_residuals <- residuals(object = res.aov.BCI)
# Run Shapiro-Wilk test
shapiro.test(x = aov_residuals)
kruskal.test(BCI ~ Year, data = df)
# Take a look at the timespan
summary(df$date)
load(file = "daily_weather_2008-2019.Rdata")
daily <- daily %>%
# Calculate derived metrics for each day
mutate(DiurnialRange = Tmax - Tmin,
MeanTemp = (Tmin + Tmax) / 2,
# Get only positive mean temperatures (for Growing degree units calculation)
PositiveMeanTemp = ifelse(MeanTemp > 0, MeanTemp, 0)) %>%
# Adjust dates and add new variables for month and years
mutate(date = as.Date(date),
month = months(date),
year = format(date, format = "%Y")) %>%
mutate(year = as.numeric(year)) %>%
# Filter only years of inventories
filter(year %in% c(2008, 2011, 2014, 2019))
# List all required years
list_of_years <- c(2008, 2011, 2014, 2019)
# Create an empty list
gdu_by_years <- list()
for (k in 1:length(list_of_years)) {
# Calculate GDUs for each year
df_temp <- daily %>%
# loop doesn't seem to work with `date` format, convert to character
mutate(date = as.character(date)) %>%
filter(year == list_of_years[[k]]) %>%
select(place, date, PositiveMeanTemp) %>%
split(.$place)
# Create an empty list
df_gdu <- list()
# Loop through the list elements (data frames with all days for the particular year,
# split by place ID)
for (i in 1:length(df_temp)) {
df_gdu[[i]] <- gdu(df_temp[[i]], "PositiveMeanTemp")
df_gdu[[i]]$place <- df_temp[[i]]$place
}
# Transform list of data frames into a single data frame
gdu_by_years[[k]] <- bind_rows(df_gdu)
}
# Transform list of data frames into a single data frame
GDU <- bind_rows(gdu_by_years) %>%
mutate(date = as.Date(date))
# Add GDU as additional variable to the daily climate data for required years
daily <- daily %>%
left_join(GDU, by = c("date", "place"))
# Drop temporary variables not longer required
rm(df_temp, df_gdu, GDU, gdu_by_years, i, k)
# List all required years
list_of_years <- c(2008, 2011, 2014, 2019)
# Create an empty list
SprOn_by_years <- list()
for (k in 1:length(list_of_years)) {
# Calculate spring onset day for each year/place
df_temp <- daily %>%
# loop doesn't seem to work with `date` format, convert to character
# mutate(date = as.character(date)) %>%
filter(year == list_of_years[[k]]) %>%
# filter(year == list_of_years[[1]]) %>%
select(place, date, MeanTemp) %>%
split(.$place)
# Create an empty list
df_SprOn <- list()
# Loop through the list elements (data frames with all days for the particular year,
# split by place ID)
for (i in 1:length(df_temp)) {
df_SprOn[[i]] <- c(place = df_temp[[i]]$place[1],
spring_onset = spring_onset_day(df_temp[[i]], "MeanTemp"))
}
# Transform list of data frames into a single data frame
SprOn_by_years[[k]] <- bind_rows(df_SprOn) %>%
mutate(year = list_of_years[k])
}
# Transform list of data frames into a single data frame
SprOn <- bind_rows(SprOn_by_years)
# Drop temporary variables not longer required
rm(df_temp, df_SprOn, SprOn_by_years, i, k)
daily %>%
mutate(day = lubridate::yday(date)) %>%
select(date, day) %>%
filter(month(date) == 3 & day(date) == 25) %>%
unique()
# Pre-natal (spring) periods only
daily %>%
filter(date >= "2008-03-25" & date < "2008-05-25") %>%
mutate(season = "spring") -> spring2008
daily %>%
filter(date >= "2011-03-25" & date < "2011-05-25") %>%
mutate(season = "spring") -> spring2011
daily %>%
filter(date >= "2014-03-25" & date < "2014-05-25") %>%
mutate(season = "spring") -> spring2014
daily %>%
filter(date >= "2019-03-25" & date < "2019-05-25") %>%
mutate(season = "spring") -> spring2019
# Post-natal (summer) periods only
daily %>%
filter(date >= "2008-05-25" & date < "2008-06-25") %>%
mutate(season = "summer") -> summer2008
daily %>%
filter(date >= "2011-05-25" & date < "2011-06-25") %>%
mutate(season = "summer") -> summer2011
daily %>%
filter(date >= "2014-05-25" & date < "2014-06-25") %>%
mutate(season = "summer") -> summer2014
daily %>%
filter(date >= "2019-05-25" & date < "2019-06-25") %>%
mutate(season = "summer") -> summer2019
daily_spring_summer <- bind_rows(spring2008, spring2011, spring2014, spring2019,
summer2008, summer2011, summer2014, summer2019)
# Remove unused temorary variables
rm(spring2008, spring2011, spring2014, spring2019,
summer2008, summer2011, summer2014, summer2019)
daily_spring_summer %>%
filter(season == "spring") %>%
select(date, year, Tmin) |>
ggplot(aes(Tmin)) +
geom_density(fill = "lightgreen",
alpha = 0.5) +
facet_wrap( ~ factor(year)) +
labs(title = "Spring",
x = "Minimal temperature") +
theme_bw()
ggsave("./figures/Tmin_2008-2019_density_Spring.png", width = 16, height = 12, units = "cm", dpi = 150)
daily_spring_summer %>%
filter(season == "summer") %>%
select(date, year, Tmin) |>
ggplot(aes(Tmin)) +
geom_density(fill = "lightgreen",
alpha = 0.5) +
facet_wrap( ~ factor(year)) +
labs(title = "Summer",
x = "Minimal temperature") +
theme_bw()
ggsave("./figures/Tmin_2008-2019_density_Apr.png", width = 16, height = 12, units = "cm", dpi = 150)
# Cumulative precipitation
daily_spring_summer %>%
group_by(season, place, year) %>%
summarise(sum(Prcp)) %>%
rename(Prcp = 4) %>%
as_tibble() -> Prcp_var
# Split spring only precipitation
Prcp_var %>%
filter(season == "spring") %>%
rename(Prcp_spr = Prcp) %>%
select(-season) -> Prcp_spring
# Split summer only precipitation
Prcp_var %>%
filter(season == "summer") %>%
rename(Prcp_summ = Prcp) %>%
select(-season) -> Prcp_summer
# Average minimum daily temperature
daily_spring_summer %>%
group_by(season, place, year) %>%
summarise(mean(Tmin)) %>%
rename(Tmin = 4) %>%
as_tibble() -> Tmin_var
# Dissect only spring minimal temperatures
Tmin_var %>%
filter(season == "spring") %>%
rename(Tmin_spr = Tmin) %>%
select(-season) -> Tmin_spring
# Dissect only spring minimal temperatures
Tmin_var %>%
filter(season == "summer") %>%
rename(Tmin_summ = Tmin) %>%
select(-season) -> Tmin_summer
# Number of days with mean temperature less or equal 10 Celsius degree
daily_spring_summer %>%
filter(season == "spring") %>%
filter(MeanTemp <= 10) %>%
count(season, place, year) %>%
as_tibble() %>%
rename(Less10_var = n) %>%
select(-season) -> Less10_var
# GDU at the end of a spring period
daily_spring_summer %>%
filter(month(date) == 5 & day(date) == 24) %>%
select(place, year, GDU) %>%
rename(GDU_spr = GDU) -> GDU_spring
# GDU at the end of a summer period
daily_spring_summer %>%
filter(month(date) == 6 & day(date) == 24) %>%
select(place, year, GDU) %>%
rename(GDU_summ = GDU) -> GDU_summer
# Merge weather covariates
Prcp_spring %>%
left_join(Prcp_summer, by = join_by(place, year)) %>%
left_join(Tmin_spring, by = join_by(place, year)) %>%
left_join(Tmin_summer, by = join_by(place, year)) %>%
left_join(Less10_var, by = join_by(place, year)) %>%
left_join(GDU_spring, by = join_by(place, year)) %>%
left_join(GDU_summer, by = join_by(place, year)) %>%
left_join(SprOn, by = join_by(place, year)) %>%
mutate(across(c(GDU_spr,
GDU_summ,
spring_onset), as.numeric)) %>%
rename(Place = place,
Year = year) %>%
mutate(Year = as.factor(Year)) -> covariates
# Drop unused variables
rm(Prcp_spring, Prcp_var, Prcp_summer, Tmin_spring, Tmin_summer, Tmin_var,
GDU_summer, GDU_spring, Less10_var, daily_spring_summer)
# Merge data
df_clim <- df %>%
left_join(covariates, by = join_by(Place, Year))
# Export climate data for further sessions
save(df_clim, file = "bats_wClimate_2008-2019.Rdata")
# Remove temporary variables
rm(barlett_BCI, barlett_Ra, covariates, daily, df, p1, p2, res.aov.BCI,
res.aov.Ra, SprOn, aov_residuals, list_of_years, check_my_var, gdu, spring_onset_day)
# Corrplot
library(corrplot)
M = cor(df_clim[, 13:ncol(df_clim)])
# corrplot(M, method = 'square', order = 'FPC', type = 'lower', diag = FALSE)
# Custom function to calculate significance level (p-values) for correlation matrix
source("./functions/sign_level.R")
# matrix of the p-value of the correlation
p.mat <- sign_level(df_clim[, 13:ncol(df_clim)])
col <- colorRampPalette(c("#BB4444", "#EE9988", "#FFFFFF", "#77AADD", "#4477AA"))
corrplot(M, method = "color", col = col(200),
type = "upper", order = "hclust",
addCoef.col = "black", # Add coefficient of correlation
tl.col="black", tl.srt = 45, #Text label color and rotation
# Combine with significance
p.mat = p.mat, sig.level = 0.05, insig = "blank",
# hide correlation coefficient on the principal diagonal
diag=FALSE
)
library(PerformanceAnalytics)
chart.Correlation(df_clim[, 13:ncol(df_clim)], histogram = TRUE, method = "pearson")
library(usdm)
# calculates vif for the variables in r
vif(df_clim[, 13:ncol(df_clim)])
# identify collinear variables that should be excluded
v1 <- vifcor(df_clim[, 13:ncol(df_clim)], th = 0.7) # th - correlation threshold
v1
# Identify less correlated set of variables
v2 <- vifstep(df_clim[, 13:ncol(df_clim)], th = 10)
v2
# Check model assumptions visually
library(performance)
# Set the formula
formula_Ra_all_covs <- "Ra ~ Prcp_spr + Prcp_summ + Tmin_spr + Tmin_summ + Less10_var + GDU_spr + GDU_summ + spring_onset"
formula_BCI_all_covs <- "BCI ~ Prcp_spr + Prcp_summ + Tmin_spr + Tmin_summ + Less10_var + GDU_spr + GDU_summ + spring_onset"
mod_Ra_all <- lm(formula = as.formula(formula_Ra_all_covs), data = df_clim)
summary(mod_Ra_all)
# png("./figures/check_model.png", width = 16, height = 12, units = "cm", res = 300)
check_model(mod_Ra_all, panel = FALSE) %>%  plot()
# dev.off()
