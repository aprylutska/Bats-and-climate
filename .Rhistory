# Compute the analysis of variance
res.aov.Ra <- aov(Ra ~ Year, data = df)
summary(res.aov.Ra)
# H0: All group means are equal
# HA: Not all group means are equal
# The F value column is the test statistic from the F test. This is the mean square of each independent variable divided by the mean square of the residuals. The larger the F value, the more likely it is that the variation caused by the independent variable is real and not due to chance.
# The Pr(>F) column is the p value of the F statistic. This shows how likely it is that the F value calculated from the test would have occurred if the null hypothesis of no difference among group means were true.
ggplot(df, aes(x = Year, y = Ra, colour = Year)) +
geom_boxplot() +
scale_color_manual(values = c("#00AFBB", "#E7B800", "#FC4E07")) +
annotate("text",        # type
x = 1, y = 49, # position
hjust = 0,     # alignment left
label = paste(paste0("F = ", round(summary(res.aov.Ra)[[1]][["F value"]][1], digits = 3)),
paste0("p = ", round(summary(res.aov.Ra)[[1]][["Pr(>F)"]][1], digits = 3)),
sep = "\n"),
family = "serif", fontface = "italic", size = 5) +
labs(title = "Differences in forearm length for Noctule bats,\nOne-way Analysis of Variance",
x = "", y = "Forearm length, mm",
colour = "") +
# scale_colour_discrete(labels = c("I. setosa", "I. versicolor", "I. virginica")) +
theme(axis.text.x = element_blank(),
axis.ticks.x = element_blank(),
# match legend key to panel.background
legend.key       = element_blank(),
legend.text = element_text(face = "italic", size = 12),
legend.position = "bottom")
# Tukey Honest Significant Differences
TukeyHSD(res.aov.Ra, conf.level = .95)
# if adjusted p-value lesser than 0.05 - pair differs significantly
# Plotting ANOVA assumptions
par(mfrow=c(1,2))
# Check ANOVA assumptions
# 1. Homogeneity of variances (The residuals versus fits plot)
plot(res.aov.Ra, 1)
# 2. Normality plot of residuals
plot(res.aov.Ra, 2)
par(mfrow=c(1,1))
# Alternatively - Shapiro-Wilk test for the residuals
# Extract the residuals
aov_residuals <- residuals(object = res.aov.Ra)
# Run Shapiro-Wilk test
shapiro.test(x = aov_residuals)
# Compute the analysis of variance
res.aov.BCI <- aov(BCI ~ Year, data = df)
summary(res.aov.BCI)
# H0: All group means are equal
# HA: Not all group means are equal
# The F value column is the test statistic from the F test. This is the mean square of each independent variable divided by the mean square of the residuals. The larger the F value, the more likely it is that the variation caused by the independent variable is real and not due to chance.
# The Pr(>F) column is the p value of the F statistic. This shows how likely it is that the F value calculated from the test would have occurred if the null hypothesis of no difference among group means were true.
ggplot(df, aes(x = Year, y = BCI, colour = Year)) +
geom_boxplot() +
scale_color_manual(values = c("#00AFBB", "#E7B800", "#FC4E07")) +
# annotate("text",  x = 1, y = -0.4,
#          label = paste0("p = ", round(summary(res.aov.BCI)[[1]][["Pr(>F)"]][1], digits = 4)),
#          family = "serif", fontface = "italic", size = 5) +
annotate("text",        # type
x = 1, y = -0.4, # position
hjust = 0,     # alignment left
label = paste(paste0("F = ", round(summary(res.aov.BCI)[[1]][["F value"]][1], digits = 3)),
paste0("p = ", round(summary(res.aov.BCI)[[1]][["Pr(>F)"]][1], digits = 3)),
sep = "\n"),
family = "serif", fontface = "italic", size = 5) +
labs(title = "Differences in forearm length for Noctule bats,\nOne-way Analysis of Variance",
x = "", y = "BCI",
colour = "") +
# scale_colour_discrete(labels = c("I. setosa", "I. versicolor", "I. virginica")) +
theme(axis.text.x = element_blank(),
axis.ticks.x = element_blank(),
# match legend key to panel.background
legend.key       = element_blank(),
legend.text = element_text(face = "italic", size = 12),
legend.position = "bottom")
# Tukey Honest Significant Differences
TukeyHSD(res.aov.BCI, conf.level = .95)
# if adjusted p-value lesser than 0.05 - pair differs significantly
# Plotting ANOVA assumptions
par(mfrow=c(1,2))
# Check ANOVA assumptions
# 1. Homogeneity of variances (The residuals versus fits plot)
plot(res.aov.BCI, 1)
# 2. Normality plot of residuals
plot(res.aov.BCI, 2)
par(mfrow=c(1,1))
# Alternatively - Shapiro-Wilk test for the residuals
# ExtBCIct the residuals
aov_residuals <- residuals(object = res.aov.BCI)
# Run Shapiro-Wilk test
shapiro.test(x = aov_residuals)
kruskal.test(BCI ~ Year, data = df)
# Take a look at the timespan
summary(df$date)
load(file = "daily_weather_2008-2019.Rdata")
daily <- daily %>%
# Calculate derived metrics for each day
mutate(DiurnialRange = Tmax - Tmin,
MeanTemp = (Tmin + Tmax) / 2,
# Get only positive mean temperatures (for Growing degree units calculation)
PositiveMeanTemp = ifelse(MeanTemp > 0, MeanTemp, 0)) %>%
# Adjust dates and add new variables for month and years
mutate(date = as.Date(date),
month = months(date),
year = format(date, format = "%Y")) %>%
mutate(year = as.numeric(year)) %>%
# Filter only years of inventories
filter(year %in% c(2008, 2011, 2014, 2019))
# List all required years
list_of_years <- c(2008, 2011, 2014, 2019)
# Create an empty list
gdu_by_years <- list()
for (k in 1:length(list_of_years)) {
# Calculate GDUs for each year
df_temp <- daily %>%
# loop doesn't seem to work with `date` format, convert to character
mutate(date = as.character(date)) %>%
filter(year == list_of_years[[k]]) %>%
select(place, date, PositiveMeanTemp) %>%
split(.$place)
# Create an empty list
df_gdu <- list()
# Loop through the list elements (data frames with all days for the particular year,
# split by place ID)
for (i in 1:length(df_temp)) {
df_gdu[[i]] <- gdu(df_temp[[i]], "PositiveMeanTemp")
df_gdu[[i]]$place <- df_temp[[i]]$place
}
# Transform list of data frames into a single data frame
gdu_by_years[[k]] <- bind_rows(df_gdu)
}
# Transform list of data frames into a single data frame
GDU <- bind_rows(gdu_by_years) %>%
mutate(date = as.Date(date))
# Add GDU as additional variable to the daily climate data for required years
daily <- daily %>%
left_join(GDU, by = c("date", "place"))
# Drop temporary variables not longer required
rm(df_temp, df_gdu, GDU, gdu_by_years, i, k)
# List all required years
list_of_years <- c(2008, 2011, 2014, 2019)
# Create an empty list
SprOn_by_years <- list()
for (k in 1:length(list_of_years)) {
# Calculate spring onset day for each year/place
df_temp <- daily %>%
# loop doesn't seem to work with `date` format, convert to character
# mutate(date = as.character(date)) %>%
filter(year == list_of_years[[k]]) %>%
# filter(year == list_of_years[[1]]) %>%
select(place, date, MeanTemp) %>%
split(.$place)
# Create an empty list
df_SprOn <- list()
# Loop through the list elements (data frames with all days for the particular year,
# split by place ID)
for (i in 1:length(df_temp)) {
df_SprOn[[i]] <- c(place = df_temp[[i]]$place[1],
spring_onset = spring_onset_day(df_temp[[i]], "MeanTemp"))
}
# Transform list of data frames into a single data frame
SprOn_by_years[[k]] <- bind_rows(df_SprOn) %>%
mutate(year = list_of_years[k])
}
# Transform list of data frames into a single data frame
SprOn <- bind_rows(SprOn_by_years)
# Drop temporary variables not longer required
rm(df_temp, df_SprOn, SprOn_by_years, i, k)
daily %>%
mutate(day = lubridate::yday(date)) %>%
select(date, day) %>%
filter(month(date) == 3 & day(date) == 25) %>%
unique()
# Pre-natal (spring) periods only
daily %>%
filter(date >= "2008-03-25" & date < "2008-05-25") %>%
mutate(season = "spring") -> spring2008
daily %>%
filter(date >= "2011-03-25" & date < "2011-05-25") %>%
mutate(season = "spring") -> spring2011
daily %>%
filter(date >= "2014-03-25" & date < "2014-05-25") %>%
mutate(season = "spring") -> spring2014
daily %>%
filter(date >= "2019-03-25" & date < "2019-05-25") %>%
mutate(season = "spring") -> spring2019
# Post-natal (summer) periods only
daily %>%
filter(date >= "2008-05-25" & date < "2008-06-25") %>%
mutate(season = "summer") -> summer2008
daily %>%
filter(date >= "2011-05-25" & date < "2011-06-25") %>%
mutate(season = "summer") -> summer2011
daily %>%
filter(date >= "2014-05-25" & date < "2014-06-25") %>%
mutate(season = "summer") -> summer2014
daily %>%
filter(date >= "2019-05-25" & date < "2019-06-25") %>%
mutate(season = "summer") -> summer2019
daily_spring_summer <- bind_rows(spring2008, spring2011, spring2014, spring2019,
summer2008, summer2011, summer2014, summer2019)
# Remove unused temorary variables
rm(spring2008, spring2011, spring2014, spring2019,
summer2008, summer2011, summer2014, summer2019)
daily_spring_summer %>%
filter(season == "spring") %>%
select(date, year, Tmin) |>
ggplot(aes(Tmin)) +
geom_density(fill = "lightgreen",
alpha = 0.5) +
facet_wrap( ~ factor(year)) +
labs(title = "Spring",
x = "Minimal temperature") +
theme_bw()
ggsave("./figures/Tmin_2008-2019_density_Spring.png", width = 16, height = 12, units = "cm", dpi = 150)
daily_spring_summer |>
filter(season == "summer") %>%
select(date, year, Tmin) |>
ggplot(aes(Tmin)) +
geom_density(fill = "lightgreen",
alpha = 0.5) +
facet_wrap( ~ factor(year)) +
labs(title = "Summer",
x = "Minimal temperature") +
theme_bw()
ggsave("./figures/Tmin_2008-2019_density_Apr.png", width = 16, height = 12, units = "cm", dpi = 150)
# Cumulative precipitation
daily_spring_summer %>%
group_by(season, place, year) %>%
summarise(sum(Prcp)) %>%
rename(Prcp = 4) %>%
as_tibble() -> Prcp_var
# Split spring only precipitation
Prcp_var %>%
filter(season == "spring") %>%
rename(Prcp_spr = Prcp) %>%
select(-season) -> Prcp_spring
# Split summer only precipitation
Prcp_var %>%
filter(season == "summer") %>%
rename(Prcp_summ = Prcp) %>%
select(-season) -> Prcp_summer
# Average minimum daily temperature
daily_spring_summer %>%
group_by(season, place, year) %>%
summarise(mean(Tmin)) %>%
rename(Tmin = 4) %>%
as_tibble() -> Tmin_var
# Dissect only spring minimal temperatures
Tmin_var %>%
filter(season == "spring") %>%
rename(Tmin_spr = Tmin) %>%
select(-season) -> Tmin_spring
# Dissect only spring minimal temperatures
Tmin_var %>%
filter(season == "summer") %>%
rename(Tmin_summ = Tmin) %>%
select(-season) -> Tmin_summer
# Number of days with mean temperature less or equal 10 Celsius degree
daily_spring_summer %>%
filter(season == "spring") %>%
filter(MeanTemp <= 10) %>%
count(season, place, year) %>%
as_tibble() %>%
rename(Less10_var = n) %>%
select(-season) -> Less10_var
# GDU at the end of a spring period
daily_spring_summer %>%
filter(month(date) == 5 & day(date) == 24) %>%
select(place, year, GDU) %>%
rename(GDU_spr = GDU) -> GDU_spring
# GDU at the end of a summer period
daily_spring_summer %>%
filter(month(date) == 6 & day(date) == 24) %>%
select(place, year, GDU) %>%
rename(GDU_summ = GDU) -> GDU_summer
# Merge weather covariates
Prcp_spring %>%
left_join(Prcp_summer, by = join_by(place, year)) %>%
left_join(Tmin_spring, by = join_by(place, year)) %>%
left_join(Tmin_summer, by = join_by(place, year)) %>%
left_join(Less10_var, by = join_by(place, year)) %>%
left_join(GDU_spring, by = join_by(place, year)) %>%
left_join(GDU_summer, by = join_by(place, year)) %>%
left_join(SprOn, by = join_by(place, year)) %>%
mutate(across(c(GDU_spr,
GDU_summ,
spring_onset), as.numeric)) %>%
rename(Place = place,
Year = year) %>%
mutate(Year = as.factor(Year)) -> covariates
# Drop unused variables
rm(Prcp_spring, Prcp_var, Prcp_summer, Tmin_spring, Tmin_summer, Tmin_var,
GDU_summer, GDU_spring, Less10_var, daily_spring_summer)
# Merge data
df_clim <- df %>%
left_join(covariates, by = join_by(Place, Year))
# Export climate data for further sessions
save(df_clim, file = "bats_wClimate_2008-2019.Rdata")
# Load previously generated data if you have started a new session
load(file = "bats_wClimate_2008-2019.Rdata")
colnames(df_clim[12:ncol(df_clim)])
colnames(df_clim[13:ncol(df_clim)])
# Set the formula
formula_Ra_all_covs <- formula(Ra ~ Prcp_spr + Prcp_summ + Tmin_spr + Tmin_summ + Less10_var + GDU_spr + GDU_summ + spring_onset)
# Set the formula
formula_Ra_all_covs <- formula(Ra ~ Prcp_spr + Prcp_summ + Tmin_spr + Tmin_summ + Less10_var + GDU_spr + GDU_summ + spring_onset)
mod_Ra_all <- lm(formula = formula, data = df_clim)
# Set the formula
formula_Ra_all_covs <- as.formula(Ra ~ Prcp_spr + Prcp_summ + Tmin_spr + Tmin_summ + Less10_var + GDU_spr + GDU_summ + spring_onset)
# Set the formula
formula_Ra_all_covs <- as.formula(Ra ~ Prcp_spr + Prcp_summ + Tmin_spr + Tmin_summ + Less10_var + GDU_spr + GDU_summ + spring_onset)
```{r}
mod_Ra_all <- lm(formula = formula, data = df_clim)
print(formula_Ra_all_covs)
mod_Ra_all <- lm(formula = as.formula(formula_Ra_all_covs), data = df_clim)
# Set the formula
formula_Ra_all_covs <- "Ra ~ Prcp_spr + Prcp_summ + Tmin_spr + Tmin_summ + Less10_var + GDU_spr + GDU_summ + spring_onset"
print(formula_Ra_all_covs)
mod_Ra_all <- lm(formula = as.formula(formula_Ra_all_covs), data = df_clim)
library(car)
install.packages("car")
library(car)
library(car)
# Calculating VIF
vif_values <- vif(model)
# Calculating VIF
vif_values <- vif(mod_Ra_all)
mod_Ra_all
summary(mod_Ra_all)
View(df_clim)
# mod_Ra_all <- lm(formula = as.formula(formula_Ra_all_covs), data = df_clim)
mod_Ra_all <- lm(Ra ~ Prcp_spr + Prcp_summ + Tmin_spr + Tmin_summ + Less10_var + GDU_spr + GDU_summ + spring_onset,
data = df_clim)
summary(mod_Ra_all)
mod_Ra_all <- lm(formula = as.formula(formula_Ra_all_covs), data = df_clim)
summary(mod_Ra_all)
# Set the formula
formula_Ra_all_covs <- "Ra ~ Prcp_spr + Prcp_summ + Tmin_spr + Tmin_summ + Less10_var + GDU_spr + GDU_summ + spring_onset"
formula_BCI_all_covs <- "BCI ~ Prcp_spr + Prcp_summ + Tmin_spr + Tmin_summ + Less10_var + GDU_spr + GDU_summ + spring_onset"
mod_Ra_all <- lm(formula = as.formula(formula_Ra_all_covs), data = df_clim)
summary(mod_Ra_all)
mod_BCI_all <- lm(formula = as.formula(formula_BCI_all_covs), data = df_clim)
summary(mod_BCI_all)
library(car)
# Calculating VIF
vif_values <- vif(mod_Ra_all)
mod_SprOn <- lm(Ra ~ spring_onset, data = df_clim)
summary(mod_SprOn)
mod_SprOn <- lm(Ra ~ GDU_spr + spring_onset, data = df_clim)
summary(mod_SprOn)
mod_SprOn <- lm(BCI ~ GDU_spr + spring_onset, data = df_clim)
summary(mod_SprOn)
install.packages("corrplot")
# Corrplots
library(corrplot)
M = cor(df_clim[, 13:ncol(df_clim)])
M
corrplot(M) # by default, method = 'circle'
corrplot(M, method = 'number') # colorful number
corrplot(M, method = 'color', order = 'alphabet')
corrplot(M, method = 'square', order = 'FPC', type = 'lower', diag = FALSE)
corrplot.mixed(M, lower = 'shade', upper = 'pie', order = 'hclust')
# Corrplots
library(corrplot)
M = cor(df_clim[, 13:ncol(df_clim)])
M
# corrplot(M) # by default, method = 'circle'
#
# corrplot(M, method = 'number') # colorful number
#
# corrplot(M, method = 'color', order = 'alphabet')
corrplot(M, method = 'square', order = 'FPC', type = 'lower', diag = FALSE)
# corrplot.mixed(M, lower = 'shade', upper = 'pie', order = 'hclust')
install.packages("PerformanceAnalytics")
library(PerformanceAnalytics)
chart.Correlation(df_clim[, 13:ncol(df_clim)], histogram = TRUE, method = "pearson")
corrplot(M, method = "color", col = col(200),
type = "upper", order = "hclust",
addCoef.col = "black", # Add coefficient of correlation
tl.col="black", tl.srt = 45, #Text label color and rotation
# Combine with significance
p.mat = p.mat, sig.level = 0.01, insig = "blank",
# hide correlation coefficient on the principal diagonal
diag=FALSE
)
col <- colorRampPalette(c("#BB4444", "#EE9988", "#FFFFFF", "#77AADD", "#4477AA"))
corrplot(M, method = "color", col = col(200),
type = "upper", order = "hclust",
addCoef.col = "black", # Add coefficient of correlation
tl.col="black", tl.srt = 45, #Text label color and rotation
# Combine with significance
p.mat = p.mat, sig.level = 0.01, insig = "blank",
# hide correlation coefficient on the principal diagonal
diag=FALSE
)
# mat : is a matrix of data
# ... : further arguments to pass to the native R cor.test function
cor.mtest <- function(mat, ...) {
mat <- as.matrix(mat)
n <- ncol(mat)
p.mat<- matrix(NA, n, n)
diag(p.mat) <- 0
for (i in 1:(n - 1)) {
for (j in (i + 1):n) {
tmp <- cor.test(mat[, i], mat[, j], ...)
p.mat[i, j] <- p.mat[j, i] <- tmp$p.value
}
}
colnames(p.mat) <- rownames(p.mat) <- colnames(mat)
p.mat
}
# matrix of the p-value of the correlation
p.mat <- cor.mtest(df_clim[, 13:ncol(df_clim)])
col <- colorRampPalette(c("#BB4444", "#EE9988", "#FFFFFF", "#77AADD", "#4477AA"))
corrplot(M, method = "color", col = col(200),
type = "upper", order = "hclust",
addCoef.col = "black", # Add coefficient of correlation
tl.col="black", tl.srt = 45, #Text label color and rotation
# Combine with significance
p.mat = p.mat, sig.level = 0.01, insig = "blank",
# hide correlation coefficient on the principal diagonal
diag=FALSE
)
corrplot(M, method = "color", col = col(200),
type = "upper", order = "hclust",
addCoef.col = "black", # Add coefficient of correlation
tl.col="black", tl.srt = 45, #Text label color and rotation
# Combine with significance
p.mat = p.mat, sig.level = 0.05, insig = "blank",
# hide correlation coefficient on the principal diagonal
diag=FALSE
)
# Corrplot
library(corrplot)
M = cor(df_clim[, 13:ncol(df_clim)])
corrplot(M, method = 'square', order = 'FPC', type = 'lower', diag = FALSE)
# Basic
pairs(df_clim[, 13:ncol(df_clim)])
# Corrplot
library(corrplot)
M = cor(df_clim[, 13:ncol(df_clim)])
# corrplot(M, method = 'square', order = 'FPC', type = 'lower', diag = FALSE)
# Custom function to calculate significance level (p-values) for correlation matrix
# mat : is a matrix of data
# ... : further arguments to pass to the native R cor.test function
cor.mtest <- function(mat, ...) {
mat <- as.matrix(mat)
n <- ncol(mat)
p.mat<- matrix(NA, n, n)
diag(p.mat) <- 0
for (i in 1:(n - 1)) {
for (j in (i + 1):n) {
tmp <- cor.test(mat[, i], mat[, j], ...)
p.mat[i, j] <- p.mat[j, i] <- tmp$p.value
}
}
colnames(p.mat) <- rownames(p.mat) <- colnames(mat)
p.mat
}
# matrix of the p-value of the correlation
p.mat <- cor.mtest(df_clim[, 13:ncol(df_clim)])
col <- colorRampPalette(c("#BB4444", "#EE9988", "#FFFFFF", "#77AADD", "#4477AA"))
corrplot(M, method = "color", col = col(200),
type = "upper", order = "hclust",
addCoef.col = "black", # Add coefficient of correlation
tl.col="black", tl.srt = 45, #Text label color and rotation
# Combine with significance
p.mat = p.mat, sig.level = 0.05, insig = "blank",
# hide correlation coefficient on the principal diagonal
diag=FALSE
)
library(PerformanceAnalytics)
chart.Correlation(df_clim[, 13:ncol(df_clim)], histogram = TRUE, method = "pearson")
library(PerformanceAnalytics)
chart.Correlation(df_clim[, 13:ncol(df_clim)], histogram = TRUE, method = "pearson")
library(car)
# Calculating VIF
vif_values <- vif(mod_Ra_all)
library(usdm)
v <- vifstep(df_clim[, 13:ncol(df_clim), th = 10)
v <- vifstep(df_clim[, 13:ncol(df_clim)], th = 10)
v
library(usdm)
vif(df_clim[, 13:ncol(df_clim)]) # calculates vif for the variables in r
v1 <- vifcor(df_clim[, 13:ncol(df_clim)], th=0.9) # identify collinear variables that should be excluded
v1
v2 <- vifstep(df_clim[, 13:ncol(df_clim)], th = 10)
v2
v1
vif(df_clim[, 13:ncol(df_clim)]) # calculates vif for the variables in r
v1
v2
library(usdm)
vif(df_clim[, 13:ncol(df_clim)]) # calculates vif for the variables in r
v1 <- vifcor(df_clim[, 13:ncol(df_clim)], th=0.9) # identify collinear variables that should be excluded
v1
v2 <- vifstep(df_clim[, 13:ncol(df_clim)], th = 10)
v2
vif(df_clim[, 13:ncol(df_clim)]) # calculates vif for the variables in r
# identify collinear variables that should be excluded
v1 <- vifcor(df_clim[, 13:ncol(df_clim)], th = 0.7) # th - correlation threshold
v1
# identify collinear variables that should be excluded
v1 <- vifcor(df_clim[, 13:ncol(df_clim)], th = 0.7) # th - correlation threshold
library(usdm)
# calculates vif for the variables in r
vif(df_clim[, 13:ncol(df_clim)])
# identify collinear variables that should be excluded
v1 <- vifcor(df_clim[, 13:ncol(df_clim)], th = 0.7) # th - correlation threshold
v1
# Identify less correlated set of variables
v2 <- vifstep(df_clim[, 13:ncol(df_clim)], th = 10)
v2
